{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dair-ai Emotion Classification Challenge\n",
    "Este notebook es para un reto propuesto en el semillero investigativo de modelos generativos, construyendo un modelo de clasificación del dataset \"Dair-ai Emotion\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch huggingface_hub ipywidgets datasets pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo un bug que no termino de entender y es por qué la linear layer en vez de devolverme algo de dimensión 1 me devuelve un tensor de la misma dimensión que indexedText :c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at C:\\Users\\juanb\\.cache\\huggingface\\datasets\\dair-ai___emotion\\split\\1.0.0\\9ce63038044ae35ec1305d998d1882fcecd70ec8 (last modified on Fri Jul 19 13:23:08 2024).\n",
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at C:\\Users\\juanb\\.cache\\huggingface\\datasets\\dair-ai___emotion\\split\\1.0.0\\9ce63038044ae35ec1305d998d1882fcecd70ec8 (last modified on Fri Jul 19 13:23:08 2024).\n",
      "Using the latest cached version of the dataset since dair-ai/emotion couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'split' at C:\\Users\\juanb\\.cache\\huggingface\\datasets\\dair-ai___emotion\\split\\1.0.0\\9ce63038044ae35ec1305d998d1882fcecd70ec8 (last modified on Fri Jul 19 13:23:08 2024).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([4]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Predictions output a Tensor(n) depending on indexedText dimension (??)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m predictions \u001b[38;5;241m=\u001b[39m emotionModel(torch\u001b[38;5;241m.\u001b[39mtensor(indexedText))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m roundedPredictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(predictions))\n\u001b[0;32m     64\u001b[0m correct \u001b[38;5;241m=\u001b[39m (roundedPredictions \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\juanb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juanb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:731\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juanb\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3224\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3221\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([4]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# We download the dair-ai/emotion dataset from huggingface\n",
    "trainDS = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
    "trainDS = trainDS.with_format(\"torch\")\n",
    "validDS = load_dataset(\"dair-ai/emotion\", split=\"validation\")\n",
    "validDS = validDS.with_format(\"torch\")\n",
    "testDS = load_dataset(\"dair-ai/emotion\", split=\"test\")\n",
    "testDS = testDS.with_format(\"torch\")\n",
    "\n",
    "# I will define my embeddings from a bag of words so...\n",
    "vocabulary = []\n",
    "for data in trainDS:\n",
    "    for word in data[\"text\"].split():\n",
    "        if word not in vocabulary:\n",
    "            vocabulary.append(word)\n",
    "indexedWords = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# This is a usage of the bag of words to translate words to numbers :)\n",
    "sentencesWIndex = [[indexedWords[word] for word in data[\"text\"].split()]\n",
    "                   for data in trainDS]\n",
    "trainDL = DataLoader(trainDS)\n",
    "\n",
    "# I will combine our embedding layer to a linear layer that outputs our emotion\n",
    "class EmotionClassificationModel(nn.Module):\n",
    "    def __init__(self, vocabularySize, embeddingDim, outputDim):\n",
    "        super().__init__()\n",
    "        # Now we need to transform sentencesWIndex into a dense vector representation\n",
    "        # Create embedding layer (matrix)\n",
    "        self.embedding = nn.Embedding(vocabularySize, embeddingDim)\n",
    "        self.linear = nn.Linear(embeddingDim, outputDim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        classification = self.linear(embedded)\n",
    "        return classification\n",
    "\n",
    "# In OpenAI text-embedding-3-small the embedding dimension is 1536, let's try 100\n",
    "emotionModel = EmotionClassificationModel(vocabularySize=len(vocabulary), embeddingDim=100, outputDim=1)\n",
    "# Define a loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# Define a optimizer that updates model parameters (Adam)\n",
    "optimizer = torch.optim.Adam(emotionModel.parameters())\n",
    "\n",
    "#Let's train our model\n",
    "for epoch in range(35):\n",
    "    epochLoss = 0\n",
    "    epochAccuracy = 0\n",
    "\n",
    "    emotionModel.train()\n",
    "    for data in trainDL:\n",
    "        label = data['label']\n",
    "        indexedText = [indexedWords[word] for word in data['text'][0].split()]\n",
    "        optimizer.zero_grad()\n",
    "        # Predictions output a Tensor(n) depending on indexedText dimension (??)\n",
    "        predictions = emotionModel(torch.tensor(indexedText)).squeeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        \n",
    "        roundedPredictions = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (roundedPredictions == label).float()\n",
    "        accuracy = correct.sum() / len(correct)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.item()\n",
    "        epochAccuracy += accuracy.item()\n",
    "\n",
    "print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" % (epoch + 1, epochLoss / len(trainDS), \n",
    "                                                  epochAccuracy / len(trainDS)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
